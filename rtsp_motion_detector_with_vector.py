"""
rtsp_motion_detector_with_vector.py
–†–µ–∞–≥–∏—Ä—É–µ—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —á–µ–ª–æ–≤–µ–∫ –ø—Ä–æ—à—ë–ª –∏–∑ –∑–æ–Ω—ã START –≤ –∑–æ–Ω—É FINISH.
–ó–æ–Ω—ã –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—é—Ç—Å—è –∫–ª–∞–≤–∏—à–∞–º–∏ –≤ –æ–¥–Ω–æ–º –æ–∫–Ω–µ (–±–µ–∑ –º—ã—à–∏).
–ï—Å—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–æ–∫–∞–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è –Ω–∞ 2 —Å–µ–∫—É–Ω–¥—ã (–æ—Ç–¥–µ–ª—å–Ω—ã–π UI-–ø–æ—Ç–æ–∫).
"""

import cv2
import numpy as np
import os
import csv
import time
import logging
import json
import simpleaudio as sa
from concurrent.futures import ThreadPoolExecutor
from ultralytics import YOLO
from queue import Queue, Empty
import threading

# ===================== –ù–ê–°–¢–†–û–ô–ö–ò =====================
SENSITIVITY = 25                  # –ø–æ—Ä–æ–≥ –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ä–∞–∑–Ω–æ—Å—Ç–∏
MIN_AREA = 800                    # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–ª–æ—â–∞–¥—å –∫–æ–Ω—Ç—É—Ä–∞ –¥–≤–∏–∂–µ–Ω–∏—è
PLAYBACK_SPEED = 8                # —á–µ–º –≤—ã—à–µ, —Ç–µ–º –º–µ–Ω—å—à–µ –Ω–∞–≥—Ä—É–∑–∫–∞ (–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π N-–π –∫–∞–¥—Ä)
SAVE_FRAMES = True
RECOGNITION_DELAY_SEC = 4         # –º–∏–Ω–∏–º—É–º —Å–µ–∫—É–Ω–¥ –º–µ–∂–¥—É —Ç—Ä–∏–≥–≥–µ—Ä–∞–º–∏/—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è–º–∏
DIRECTION_TIMEOUT_SEC = 8         # —Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥ –∂–¥—ë–º FINISH –ø–æ—Å–ª–µ –≤—Ö–æ–¥–∞ –≤ START
OUTPUT_FILE = "rtsp_motions_log.csv"
FRAMES_DIR = "rtsp_motion_frames"
ZONES_FILE = "zones.json"
MAX_THREADS = 4
YOLO_MODEL = "yolov8n.pt"
CONF_THRESHOLD = 0.7
ALARM_FILE = "icq.wav"
TARGET_CLASSES = ["person"]       # –¥–æ–±–∞–≤—å "cat","dog" –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏

# –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –∑–æ–Ω –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
PREVIEW_ZONES = True
PREVIEW_ZONES_SECONDS = 2

# UI –æ–∫–Ω–æ (ASCII-only, —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –Ω–∞ Windows)
SETUP_WINDOW = "ZONE_SETUP"
VIEW_WINDOW = "VIEWER"

# –õ–æ–≥–≥–µ—Ä
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    handlers=[
        logging.FileHandler("motion_vector_debug.log", encoding="utf-8"),
        logging.StreamHandler()
    ]
)

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
os.makedirs(FRAMES_DIR, exist_ok=True)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Ñ–ª–∞–≥–∏
PLAY_ALARM = False
SHOW_WINDOW = True  # –≤–∫–ª—é—á–∞–µ–º UI-–ø–æ—Ç–æ–∫ –ø–æ–∫–∞–∑–∞ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π

# <<< –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
_state_lock = threading.Lock()
OP_MODE = "Primed"     # "Primed" ‚Äî —Ä–∞–±–æ—Ç–∞–µ—Ç; "Idle" ‚Äî –ø–∞—É–∑–∞ —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤
VIEW_ENABLED = True    # –ø–æ–∫–∞–∑ –∫–∞–¥—Ä–æ–≤ –≤–∫–ª—é—á—ë–Ω/–≤—ã–∫–ª—é—á–µ–Ω –≥–æ—Ä—è—á–µ–π –∫–ª–∞–≤–∏—à–µ–π
_last_trigger_mark_until = 0.0  # –¥–æ –∫–∞–∫–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –ø–∏—Å–∞—Ç—å "TRIGGERED" –≤ —Å—Ç–∞—Ç—É—Å–µ

def _set_mode(new_mode: str):
    global OP_MODE
    with _state_lock:
        OP_MODE = new_mode

def _get_mode() -> str:
    with _state_lock:
        return OP_MODE

def _toggle_mode():
    with _state_lock:
        global OP_MODE
        OP_MODE = "Idle" if OP_MODE == "Primed" else "Primed"
        return OP_MODE

def _toggle_alarm():
    global PLAY_ALARM
    with _state_lock:
        PLAY_ALARM = not PLAY_ALARM
        return PLAY_ALARM

def _toggle_view():
    global VIEW_ENABLED
    with _state_lock:
        VIEW_ENABLED = not VIEW_ENABLED
        return VIEW_ENABLED

def _mark_triggered(duration_sec=2.0):
    global _last_trigger_mark_until
    with _state_lock:
        _last_trigger_mark_until = time.time() + duration_sec

def _status_line():
    with _state_lock:
        mode = OP_MODE
        alarm = "ON" if PLAY_ALARM else "OFF"
        view = "ON" if VIEW_ENABLED else "OFF"
        trig = "TRIGGERED" if time.time() < _last_trigger_mark_until else ""
    parts = [f"Mode: {mode}", f"Alarm: {alarm}", f"Display: {view}"]
    if trig:
        parts.append(trig)
    return " | ".join(parts)

# YOLO
logging.info("üì¶ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å YOLOv8...")
model = YOLO(YOLO_MODEL)

# CSV –∑–∞–≥–æ–ª–æ–≤–æ–∫
if not os.path.exists(OUTPUT_FILE):
    with open(OUTPUT_FILE, mode="w", newline="", encoding="utf-8") as f:
        csv.writer(f).writerow(["camera", "timestamp", "class", "confidence"])

# ===================== –£–¢–ò–õ–ò–¢–´ =====================
def play_alarm():
    if not PLAY_ALARM:
        return
    try:
        sa.WaveObject.from_wave_file(ALARM_FILE).play()
    except Exception as e:
        logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∏–≥—Ä–∞—Ç—å —Å–∏–≥–Ω–∞–ª: {e}")

def now_ts() -> str:
    return time.strftime("%Y-%m-%d %H:%M:%S")

def date_dir() -> str:
    d = time.strftime("%Y%m%d")
    p = os.path.join(FRAMES_DIR, d)
    os.makedirs(p, exist_ok=True)
    return p

def clamp_rect(x1, y1, x2, y2, w, h):
    x1, x2 = sorted((int(x1), int(x2)))
    y1, y2 = sorted((int(y1), int(y2)))
    x1 = max(0, min(x1, w - 1));  x2 = max(0, min(x2, w - 1))
    y1 = max(0, min(y1, h - 1));  y2 = max(0, min(y2, h - 1))
    return x1, y1, x2, y2

def in_zone(point, rect):
    if not rect or len(rect) != 4:
        return False
    cx, cy = point
    x1, y1, x2, y2 = rect
    x1, x2 = sorted((x1, x2)); y1, y2 = sorted((y1, y2))
    return x1 <= cx <= x2 and y1 <= cy <= y2

# ===================== –û–ö–ù–û-–ü–†–û–°–ú–û–¢–†–©–ò–ö (UI-–ø–æ—Ç–æ–∫) =====================
_view_queue: "Queue[np.ndarray]" = Queue(maxsize=8)
_view_stop = threading.Event()

def viewer_thread():
    cv2.namedWindow(VIEW_WINDOW, cv2.WINDOW_NORMAL)
    last_img = None
    last_time = 0.0
    hold_sec = 2.0  # –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞–¥—Ä 2 —Å–µ–∫—É–Ω–¥—ã

    while not _view_stop.is_set():
        # –≥–æ—Ä. –∫–ª–∞–≤–∏—à–∏ —á–∏—Ç–∞–µ–º —Ç—É—Ç –∂–µ
        key = cv2.waitKey(1) & 0xFF
        if key in (ord('q'), ord('Q')):
            _view_stop.set()
            break
        elif key == 32:  # SPACE ‚Äî toggle mode Idle/Primed
            new_mode = _toggle_mode()
            logging.info(f"üîÑ Mode ‚Üí {new_mode}")
        elif key in (ord('a'), ord('A')):  # A ‚Äî toggle alarm
            s = _toggle_alarm()
            logging.info(f"üîî Alarm {'ON' if s else 'OFF'}")
        elif key in (ord('w'), ord('W')):  # W ‚Äî toggle display
            s = _toggle_view()
            logging.info(f"üñºÔ∏è Display {'ON' if s else 'OFF'}")
            # –Ω–µ –∑–∞–∫—Ä—ã–≤–∞–µ–º –æ–∫–Ω–æ, –∞ –ø—Ä–æ—Å—Ç–æ –≥–∞—Å–∏–º –≤—ã–≤–æ–¥ ‚Äî —Ç–∞–∫ hotkeys –ø—Ä–æ–¥–æ–ª–∂–∞—é—Ç —Ä–∞–±–æ—Ç–∞—Ç—å

        # –∑–∞–±–∏—Ä–∞–µ–º –Ω–æ–≤—ã–π –∫–∞–¥—Ä –µ—Å–ª–∏ –µ—Å—Ç—å
        try:
            img = _view_queue.get(timeout=0.02)
            last_img = img
            last_time = time.time()
        except Empty:
            pass

        # –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ö–æ–ª—Å—Ç–∞
        if last_img is not None and VIEW_ENABLED:
            canvas = last_img.copy()
            # –Ω–∞–ª–æ–∂–∏–º —Å—Ç—Ä–æ–∫—É —Å—Ç–∞—Ç—É—Å–∞ –ø–æ–≤–µ—Ä—Ö –ø—Ä–∏—Å–ª–∞–Ω–Ω–æ–≥–æ –∫–∞–¥—Ä–∞
            cv2.putText(canvas, _status_line(), (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (20, 230, 20), 2)
            cv2.imshow(VIEW_WINDOW, canvas)
            # —Å–±—Ä–æ—Å —á–µ—Ä–µ–∑ hold_sec
            if time.time() - last_time > hold_sec:
                last_img = None
        else:
            blank = np.zeros((320, 560, 3), dtype=np.uint8)
            msg = "Display OFF" if not VIEW_ENABLED else "Waiting for events..."
            cv2.putText(blank, f"{msg}", (15, 120),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (180, 180, 180), 2)
            cv2.putText(
                blank,
                "Hotkeys: Q=quit | SPACE=Idle/Primed | A=alarm on/off | W=display on/off",
                (15, 160), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (180, 180, 180), 1
            )
            # —Ç–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å
            cv2.putText(blank, _status_line(), (15, 40),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (20, 230, 20), 2)
            cv2.imshow(VIEW_WINDOW, blank)

    cv2.destroyWindow(VIEW_WINDOW)

def show_frame(frame, camera_name, zones=None, center=None, label=None):
    """–û—Ç–ø—Ä–∞–≤–∏—Ç—å –∫–∞–¥—Ä –≤ –æ–∫–Ω–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ (UI-–ø–æ—Ç–æ–∫ —Å–∞–º –ø–æ–∫–∞–∂–µ—Ç –µ–≥–æ ~2 —Å–µ–∫)."""
    out = frame.copy()
    if zones:
        z = zones.get("start")
        if z: cv2.rectangle(out, (z[0], z[1]), (z[2], z[3]), (0, 180, 0), 2)
        z = zones.get("finish")
        if z: cv2.rectangle(out, (z[0], z[1]), (z[2], z[3]), (0, 0, 255), 2)
    if center:
        cv2.circle(out, center, 6, (255, 255, 0), -1)
    # –≤–µ—Ä—Ö–Ω—è—è —Å—Ç—Ä–æ–∫–∞
    text = f"{camera_name} | {label or ''} | {now_ts()}"
    cv2.putText(out, text, (10, 28), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (20, 230, 20), 2)
    # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Å—Ç–∞—Ç—É—Å–∞
    cv2.putText(out, _status_line(), (10, 58), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 200, 200), 2)

    # –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ–º—Å—è –Ω–∞ –ø–æ–ª–Ω–æ–π –æ—á–µ—Ä–µ–¥–∏
    try:
        _view_queue.put_nowait(out)
    except:
        pass

# ===================== –ù–ê–°–¢–†–û–ô–ö–ê –ó–û–ù (–ö–õ–ê–í–ò–®–ê–ú–ò) =====================
def select_zone_keyboard(frame, title, max_w=1280, max_h=720):
    """
    –ö–ª–∞–≤–∏–∞—Ç—É—Ä–Ω—ã–π –º–∞—Å—Ç–µ—Ä –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫–∞ –≤ –æ–¥–Ω–æ–º –æ–∫–Ω–µ SETUP_WINDOW.
    –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ:
      W/S/A/D ‚Äî —Å–¥–≤–∏–≥ –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫–∞ (10px)
      I/K ‚Äî —É–≤–µ–ª–∏—á–∏—Ç—å/—É–º–µ–Ω—å—à–∏—Ç—å –≤—ã—Å–æ—Ç—É (10px)
      L/J ‚Äî —É–≤–µ–ª–∏—á–∏—Ç—å/—É–º–µ–Ω—å—à–∏—Ç—å —à–∏—Ä–∏–Ω—É (10px)
      R ‚Äî —Å–±—Ä–æ—Å (–ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫ ~40% –ø–æ —Ü–µ–Ω—Ç—Ä—É)
      Enter ‚Äî –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å, Esc ‚Äî –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç [x1,y1,x2,y2] –∏–ª–∏ None.
    """
    h, w = frame.shape[:2]
    scale = min(1.0, max_w / float(w), max_h / float(h))
    disp = frame if scale == 1.0 else cv2.resize(frame, (int(w * scale), int(h * scale)))

    # –Ω–∞—á–∞–ª—å–Ω—ã–π –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫ ‚Äî 40% –∫–∞–¥—Ä–∞ –ø–æ —Ü–µ–Ω—Ç—Ä—É
    cw, ch = int(disp.shape[1] * 0.4), int(disp.shape[0] * 0.4)
    cx, cy = disp.shape[1] // 2, disp.shape[0] // 2
    x1, y1 = cx - cw // 2, cy - ch // 2
    x2, y2 = cx + cw // 2, cy + ch // 2

    cv2.namedWindow(SETUP_WINDOW, cv2.WINDOW_NORMAL)

    while True:
        canvas = disp.copy()
        cv2.rectangle(canvas, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(canvas, f"{title}", (10, 28), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (50, 220, 50), 2)
        cv2.putText(canvas, "W/S/A/D move | I/K height +/- | L/J width +/- | R reset | Enter OK | Esc skip",
                    (10, canvas.shape[0]-15), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (255,255,255), 2)
        cv2.imshow(SETUP_WINDOW, canvas)

        key = cv2.waitKey(10) & 0xFF
        if key in (13, 10):  # Enter
            break
        if key == 27:       # Esc
            cv2.destroyWindow(SETUP_WINDOW)
            return None

        step = 10
        if key in (ord('w'), ord('W')): y1 -= step; y2 -= step
        if key in (ord('s'), ord('S')): y1 += step; y2 += step
        if key in (ord('a'), ord('A')): x1 -= step; x2 -= step
        if key in (ord('d'), ord('D')): x1 += step; x2 += step
        if key in (ord('i'), ord('I')): y1 -= step; y2 += step
        if key in (ord('k'), ord('K')): y1 += step; y2 -= step
        if key in (ord('l'), ord('L')): x1 -= step; x2 += step
        if key in (ord('j'), ord('J')): x1 += step; x2 -= step
        if key in (ord('r'), ord('R')):
            cw, ch = int(disp.shape[1] * 0.4), int(disp.shape[0] * 0.4)
            cx, cy = disp.shape[1] // 2, disp.shape[0] // 2
            x1, y1 = cx - cw // 2, cy - ch // 2
            x2, y2 = cx + cw // 2, cy + ch // 2

        # –≥—Ä–∞–Ω–∏—Ü—ã –æ–∫–Ω–∞
        x1 = max(0, min(x1, disp.shape[1]-2)); x2 = max(1, min(x2, disp.shape[1]-1))
        y1 = max(0, min(y1, disp.shape[0]-2)); y2 = max(1, min(y2, disp.shape[0]-1))

    cv2.destroyWindow(SETUP_WINDOW)

    # –ø–µ—Ä–µ—Å—á—ë—Ç –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
    sx, sy = int(x1 / scale), int(y1 / scale)
    ex, ey = int(x2 / scale), int(y2 / scale)
    sx, sy, ex, ey = clamp_rect(sx, sy, ex, ey, w, h)
    return [sx, sy, ex, ey]

def configure_zones(cameras: dict) -> dict:
    zones = {}
    for name, url in cameras.items():
        logging.info(f"‚öô –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∑–æ–Ω –¥–ª—è –∫–∞–º–µ—Ä—ã '{name}'")
        cap = cv2.VideoCapture(url, cv2.CAP_FFMPEG)
        if not cap.isOpened():
            logging.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ {name}, –ø—Ä–æ–ø—É—Å–∫–∞—é.")
            zones[name] = {}
            continue
        ok, frame = cap.read()
        cap.release()
        if not ok or frame is None:
            logging.error(f"‚ùå –ù–µ—Ç –∫–∞–¥—Ä–∞ —Å {name}, –ø—Ä–æ–ø—É—Å–∫–∞—é.")
            zones[name] = {}
            continue

        start_rect = select_zone_keyboard(frame, f"{name}: START")
        finish_rect = select_zone_keyboard(frame, f"{name}: FINISH")
        if start_rect and finish_rect:
            zones[name] = {"start": start_rect, "finish": finish_rect}
        else:
            zones[name] = {}
            logging.warning(f"‚ö† –ó–æ–Ω—ã –¥–ª—è {name} –∑–∞–¥–∞–Ω—ã –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é.")

    with open(ZONES_FILE, "w", encoding="utf-8") as f:
        json.dump(zones, f, indent=2, ensure_ascii=False)
    logging.info("‚úÖ –ó–æ–Ω—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ zones.json")
    cv2.destroyAllWindows()
    return zones

def preview_zones(cameras: dict, zones: dict):
    if not PREVIEW_ZONES: return
    for name, url in cameras.items():
        z = zones.get(name) or {}
        if "start" not in z or "finish" not in z:
            continue
        cap = cv2.VideoCapture(url, cv2.CAP_FFMPEG)
        ok, frame = cap.read()
        cap.release()
        if not ok or frame is None:
            continue
        sx1, sy1, sx2, sy2 = z["start"];  fx1, fy1, fx2, fy2 = z["finish"]
        cv2.rectangle(frame, (sx1, sy1), (sx2, sy2), (0, 180, 0), 2)
        cv2.rectangle(frame, (fx1, fy1), (fx2, fy2), (0, 0, 255), 2)
        cv2.putText(frame, "START", (sx1, max(0, sy1-8)), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,180,0), 2)
        cv2.putText(frame, "FINISH", (fx1, max(0, fy1-8)), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,255), 2)
        win = f"PREVIEW_{name}"
        cv2.namedWindow(win, cv2.WINDOW_NORMAL)
        cv2.imshow(win, frame)
        t_end = time.time() + PREVIEW_ZONES_SECONDS
        while time.time() < t_end:
            if (cv2.waitKey(50) & 0xFF) == 27:  # Esc ‚Äî –¥–æ—Å—Ä–æ—á–Ω–æ
                break
        cv2.destroyWindow(win)
    cv2.destroyAllWindows()

# ===================== –î–ï–¢–ï–ö–¢–û–† =====================
def detect_motion_and_objects(camera_name, rtsp_url, zones_for_cam):
    """
    - –¥–µ—Ç–µ–∫—Ü–∏—è –¥–≤–∏–∂–µ–Ω–∏—è –ø–æ —Ä–∞–∑–Ω–æ—Å—Ç–∏;
    - –ø—Ä–∏ –¥–≤–∏–∂–µ–Ω–∏–∏ ‚Äî YOLO;
    - —Ü–µ–Ω—Ç—Ä–æ–∏–¥ —á–µ–ª–æ–≤–µ–∫–∞ -> –ª–æ–≥–∏–∫–∞ START‚ÜíFINISH –≤ DIRECTION_TIMEOUT_SEC.
    –ü–æ–∫–∞–∑ –∫–∞–¥—Ä–∞ ‚Äî —á–µ—Ä–µ–∑ show_frame (UI-–ø–æ—Ç–æ–∫).
    """
    try:
        cap = cv2.VideoCapture(rtsp_url, cv2.CAP_FFMPEG)
        if not cap.isOpened():
            logging.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ {camera_name}")
            return

        ret, frame1 = cap.read()
        ret2, frame2 = cap.read()
        if not ret or not ret2:
            logging.error(f"‚ùå –ù–µ—Ç —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö –∫–∞–¥—Ä–æ–≤ —É {camera_name}")
            cap.release()
            return

        last_trigger_time = 0.0
        stage = "idle"
        primed_since = 0.0
        frame_count = 0

        while True:
            # –ø—Ä–æ–ø—É—Å–∫–∏ –∫–∞–¥—Ä–æ–≤ —Ä–∞–¥–∏ CPU
            if frame_count % PLAYBACK_SPEED != 0:
                frame1 = frame2
                if not cap.grab(): break
                ok, frame2 = cap.retrieve()
                if not ok: break
                frame_count += 1
                continue

            # –î–µ—Ç–µ–∫—Ü–∏—è –¥–≤–∏–∂–µ–Ω–∏—è (–Ω–∞ —É–º–µ–Ω—å—à–µ–Ω–Ω—ã—Ö –∫–∞–¥—Ä–∞—Ö –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏)
            small1 = cv2.resize(frame1, (640, 360))
            small2 = cv2.resize(frame2, (640, 360))
            diff = cv2.absdiff(small1, small2)
            gray = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)
            blur = cv2.GaussianBlur(gray, (5, 5), 0)
            _, thresh = cv2.threshold(blur, SENSITIVITY, 255, cv2.THRESH_BINARY)
            dilated = cv2.dilate(thresh, None, iterations=3)
            contours, _ = cv2.findContours(dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
            motion_detected = any(cv2.contourArea(c) >= MIN_AREA for c in contours)

            if motion_detected:
                results = model(frame2, verbose=False)[0]
                # —Å–∞–º—ã–π —É–≤–µ—Ä–µ–Ω–Ω—ã–π —Ç–∞—Ä–≥–µ—Ç
                best = None
                for box in results.boxes:
                    cls_id = int(box.cls[0])
                    class_name = results.names[cls_id]
                    conf = float(box.conf[0])
                    if class_name in TARGET_CLASSES and conf >= CONF_THRESHOLD:
                        x1, y1, x2, y2 = map(int, box.xyxy[0])
                        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2
                        if best is None or conf > best["conf"]:
                            best = {"class": class_name, "conf": conf, "center": (cx, cy)}

                if best:
                    cx, cy = best["center"]
                    now = time.time()
                    start_rect = zones_for_cam.get("start")
                    finish_rect = zones_for_cam.get("finish")

                    # <<< –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º: –µ—Å–ª–∏ Idle ‚Äî –Ω–µ —Ç—Ä–∏–≥–≥–µ—Ä–∏–º (–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å–ª–µ–¥–∏—Ç—å –∑–∞ —Å—Ç–∞–¥–∏–µ–π)
                    active = (_get_mode() == "Primed")

                    if start_rect and finish_rect:
                        if stage == "idle":
                            if in_zone((cx, cy), start_rect):
                                stage = "primed"
                                primed_since = now
                                logging.info(f"üü¢ {camera_name}: –≤—Ö–æ–¥ –≤ START, –æ–∂–∏–¥–∞—é FINISH {DIRECTION_TIMEOUT_SEC}s")
                        elif stage == "primed":
                            if now - primed_since > DIRECTION_TIMEOUT_SEC:
                                stage = "idle"
                            elif in_zone((cx, cy), finish_rect):
                                if active and (now - last_trigger_time) >= RECOGNITION_DELAY_SEC:
                                    ts = now_ts()
                                    logging.info(f"‚úÖ {camera_name}: START‚ÜíFINISH, {best['class']} ({best['conf']:.2f}), {ts}")
                                    play_alarm()
                                    _mark_triggered()  # <<< –ø–æ–¥—Å–≤–µ—Ç–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ TRIGGERED
                                    # –ø–æ–∫–∞–∑ –≤ UI-–æ–∫–Ω–µ
                                    show_frame(frame2, camera_name, zones_for_cam, (cx, cy), "START‚ÜíFINISH")
                                    # —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                                    if SAVE_FRAMES:
                                        out = frame2.copy()
                                        sx1, sy1, sx2, sy2 = start_rect
                                        fx1, fy1, fx2, fy2 = finish_rect
                                        cv2.rectangle(out, (sx1, sy1), (sx2, sy2), (0, 180, 0), 2)
                                        cv2.rectangle(out, (fx1, fy1), (fx2, fy2), (0, 0, 255), 2)
                                        cv2.circle(out, (cx, cy), 6, (255, 255, 0), -1)
                                        fname = f"{camera_name}_{ts.replace(':','-')}_{best['class']}.jpg"
                                        cv2.imwrite(os.path.join(date_dir(), fname), out)
                                    with open(OUTPUT_FILE, "a", newline="", encoding="utf-8") as f:
                                        csv.writer(f).writerow([camera_name, ts, best["class"], f"{best['conf']:.2f}"])
                                    last_trigger_time = now
                                stage = "idle"
                    else:
                        # –∑–æ–Ω –Ω–µ—Ç ‚Äî –æ–±—ã—á–Ω—ã–π —Ç—Ä–∏–≥–≥–µ—Ä —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π
                        if active and (time.time() - last_trigger_time) >= RECOGNITION_DELAY_SEC:
                            ts = now_ts()
                            logging.info(f"‚Ñπ {camera_name}: –¥–µ—Ç–µ–∫—Ü–∏—è {best['class']} ({best['conf']:.2f}), {ts}")
                            play_alarm()
                            _mark_triggered()
                            show_frame(frame2, camera_name, None, (cx, cy), "DETECTED")
                            if SAVE_FRAMES:
                                fname = f"{camera_name}_{ts.replace(':','-')}_{best['class']}.jpg"
                                cv2.imwrite(os.path.join(date_dir(), fname), frame2)
                            with open(OUTPUT_FILE, "a", newline="", encoding="utf-8") as f:
                                csv.writer(f).writerow([camera_name, ts, best["class"], f"{best['conf']:.2f}"])
                            last_trigger_time = time.time()

            # —Å–ª–µ–¥—É—é—â–∏–π –∫–∞–¥—Ä
            frame1 = frame2
            if not cap.grab(): break
            ok, frame2 = cap.retrieve()
            if not ok: break
            frame_count += 1

        cap.release()

    except Exception as e:
        logging.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {camera_name}: {e}")

# ===================== MAIN =====================
def main():
    global PLAY_ALARM, SHOW_WINDOW

    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞–º–µ—Ä
    with open("cameras.json", "r", encoding="utf-8") as f:
        cameras = json.load(f)
    if not cameras:
        logging.error("‚ùå cameras.json –ø—É—Å—Ç–æ–π.")
        return
    if len(cameras) > MAX_THREADS:
        logging.warning(f"‚ö† –ö–∞–º–µ—Ä {len(cameras)}, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è <= {MAX_THREADS}.")

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∑–æ–Ω (–∞–≤—Ç–æ)
    need_setup = True
    zones = {}
    if os.path.exists(ZONES_FILE):
        try:
            with open(ZONES_FILE, "r", encoding="utf-8") as f:
                zones = json.load(f)
            need_setup = any(
                (name not in zones) or ("start" not in zones.get(name, {})) or ("finish" not in zones.get(name, {}))
                for name in cameras.keys()
            )
        except Exception:
            need_setup = True

    if need_setup:
        logging.info("‚öôÔ∏è –ó–æ–Ω—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã/–Ω–µ–ø–æ–ª–Ω—ã–µ ‚Äî –∑–∞–ø—É—Å–∫–∞—é –º–∞—Å—Ç–µ—Ä –Ω–∞—Å—Ç—Ä–æ–π–∫–∏‚Ä¶")
        zones = configure_zones(cameras)
    else:
        logging.info("‚úÖ –ù–∞–π–¥–µ–Ω—ã —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ –∑–æ–Ω—ã ‚Äî –º–∞—Å—Ç–µ—Ä –ø—Ä–æ–ø—É—â–µ–Ω.")

    # –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –∑–æ–Ω
    preview_zones(cameras, zones)

    # –í–æ–ø—Ä–æ—Å—ã (–ø–æ—Å–ª–µ –º–∞—Å—Ç–µ—Ä–∞)
    try:
        PLAY_ALARM = input("–ü—Ä–æ–∏–≥—Ä—ã–≤–∞—Ç—å –∑–≤—É–∫–æ–≤–æ–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ? (y/n): ").strip().lower() == "y"
    except Exception:
        PLAY_ALARM = False
    try:
        SHOW_WINDOW = input("–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –∫–∞–¥—Ä—ã –ø—Ä–∏ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–∏ (2 —Å–µ–∫)? (y/n): ").strip().lower() == "y"
    except Exception:
        SHOW_WINDOW = True

    # UI-–ø–æ—Ç–æ–∫ (—Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω, –±–µ–∑ imshow –∏–∑ —Ä–∞–±–æ—á–∏—Ö –ø–æ—Ç–æ–∫–æ–≤)
    ui_thread = None
    if SHOW_WINDOW:
        _view_stop.clear()
        ui_thread = threading.Thread(target=viewer_thread, daemon=True)
        ui_thread.start()

    logging.info("üî• –ì–æ—Ä—è—á–∏–µ –∫–ª–∞–≤–∏—à–∏ –≤ –æ–∫–Ω–µ VIEWER: Q=–≤—ã—Ö–æ–¥ | SPACE=Idle/Primed | A=–∑–≤—É–∫ ON/OFF | W=–ø–æ–∫–∞–∑ ON/OFF")
    logging.info(f"üîç –ö–∞–º–µ—Ä: {len(cameras)}. –ó–∞–ø—É—Å–∫–∞—é –≤ {min(len(cameras), MAX_THREADS)} –ø–æ—Ç–æ–∫(–∞—Ö)‚Ä¶")

    # –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    try:
        with ThreadPoolExecutor(max_workers=min(len(cameras), MAX_THREADS)) as executor:
            futures = []
            for name, url in cameras.items():
                z = zones.get(name, {})
                futures.append(executor.submit(detect_motion_and_objects, name, url, z))
            # –∂–¥—ë–º –≤—Å–µ –ø–æ—Ç–æ–∫–∏
            for f in futures:
                f.result()
    finally:
        # –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∑–∞–∫—Ä—ã—Ç—å UI-–ø–æ—Ç–æ–∫
        _view_stop.set()
        try:
            if ui_thread:
                ui_thread.join(timeout=2)
        except Exception:
            pass
        cv2.destroyAllWindows()

    logging.info("‚úÖ –†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

if __name__ == "__main__":
    main()
